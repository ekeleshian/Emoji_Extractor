{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Emoji\n",
    "\n",
    "\n",
    "Parse emoji into six categories:\n",
    "1. Person\n",
    "2. Place\n",
    "3. Thing\n",
    "4. Time\n",
    "5. Activity\n",
    "6. Mood\n",
    "\n",
    "\n",
    "Libraries:\n",
    "- emoji  https://pypi.org/project/emoji/\n",
    "- Hug http://www.hug.rest/\n",
    "\n",
    "#### Task 1:\n",
    "Given the set of emojis in emoji package.\n",
    "1. Parse each into one of the above six categories\n",
    "2. Show this work using python3 in this notebook\n",
    "3. Please document\n",
    "\n",
    "#### Task 2:\n",
    "Use the short python code snippet in app.py.\n",
    "Add code to:\n",
    "1. take in a string\n",
    "2. find if there are any emojis\n",
    "3. return a dict mapping any found emoji to their category type\n",
    "\n",
    "#### Final Notes:\n",
    "- This should be done using Python3\n",
    "- Consider this a deadline project. I just want a useable solution. It won't be perfect.\n",
    "- Please explain your process.\n",
    "- Try to limit yourself to 4 hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Step: Exploring the emoji data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni = set(emoji.EMOJI_UNICODE.keys())\n",
    "uni_ali = set(emoji.EMOJI_ALIAS_UNICODE.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uni_ali - uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `EMOJI_ALIAS_UNICODE` includes all of `EMOJI_UNICODE` emojis plus 800 more, the alias dictionary will only be utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_keys = list(emoji.EMOJI_ALIAS_UNICODE.keys())\n",
    "max_len = max([len(k) for k in arr_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":Cambodia:                                                      🇰🇭\n",
      ":France:                                                        🇫🇷\n",
      ":Japanese_open_for_business_button:                             🈺\n",
      ":Mrs._Claus:                                                    🤶\n",
      ":Russia:                                                        🇷🇺\n",
      ":TOP_arrow:                                                     🔝\n",
      ":airplane:                                                      ✈\n",
      ":backhand_index_pointing_left_medium_skin_tone:                 👈🏽\n",
      ":bellhop_bell:                                                  🛎\n",
      ":bowling:                                                       🎳\n",
      ":candle:                                                        🕯\n",
      ":clapping_hands_medium_skin_tone:                               👏🏽\n",
      ":cow:                                                           🐮\n",
      ":detective:                                                     🕵\n",
      ":eight-spoked_asterisk:                                         ✳\n",
      ":fallen_leaf:                                                   🍂\n",
      ":fishing_pole:                                                  🎣\n",
      ":gem_stone:                                                     💎\n",
      ":hand_with_fingers_splayed:                                     🖐\n",
      ":hotel:                                                         🏨\n",
      ":kimono:                                                        👘\n",
      ":llama:                                                         🦙\n",
      ":man_biking_medium_skin_tone:                                   🚴🏽‍♂️\n",
      ":man_elf:                                                       🧝‍♂️\n",
      ":man_gesturing_OK_dark_skin_tone:                               🙆🏿‍♂️\n",
      ":man_in_suit_levitating_medium-dark_skin_tone:                  🕴🏾\n",
      ":man_office_worker_dark_skin_tone:                              👨🏿‍💼\n",
      ":man_running_medium-dark_skin_tone:                             🏃🏾‍♂️\n",
      ":man_technologist_medium_skin_tone:                             👨🏽‍💻\n",
      ":mermaid_medium-dark_skin_tone:                                 🧜🏾‍♀️\n",
      ":mouse:                                                         🐭\n",
      ":ogre:                                                          👹\n",
      ":ox:                                                            🐂\n",
      ":person_bowing_dark_skin_tone:                                  🙇🏿\n",
      ":person_getting_massage_light_skin_tone:                        💆🏻\n",
      ":person_playing_handball_medium-light_skin_tone:                🤾🏼\n",
      ":person_taking_bath:                                            🛀\n",
      ":poodle:                                                        🐩\n",
      ":raised_back_of_hand:                                           🤚\n",
      ":rice_ball:                                                     🍙\n",
      ":selfie_medium-dark_skin_tone:                                  🤳🏾\n",
      ":smiling_face_with_sunglasses:                                  😎\n",
      ":steaming_bowl:                                                 🍜\n",
      ":three_o’clock:                                                 🕒\n",
      ":two_women_holding_hands:                                       👭\n",
      ":waving_hand_light_skin_tone:                                   👋🏻\n",
      ":woman_biking_light_skin_tone:                                  🚴🏻‍♀️\n",
      ":woman_detective_medium-dark_skin_tone:                         🕵🏾‍♀️\n",
      ":woman_gesturing_NO_medium-light_skin_tone:                     🙅🏼‍♀️\n",
      ":woman_judge:                                                   👩‍⚖️\n",
      ":woman_pilot_medium-light_skin_tone:                            👩🏼‍✈️\n",
      ":woman_shrugging:                                               🤷‍♀️\n",
      ":woman_vampire_light_skin_tone:                                 🧛🏻‍♀️\n",
      ":Åland_Islands:                                                 🇦🇽\n",
      ":arrow_forward:                                                 ▶\n",
      ":clock330:                                                      🕞\n",
      ":earth_africa:                                                  🌍\n",
      ":flag_for_Antarctica:                                           🇦🇶\n",
      ":flag_for_Costa_Rica:                                           🇨🇷\n",
      ":flag_for_India:                                                🇮🇳\n",
      ":flag_for_Namibia:                                              🇳🇦\n",
      ":flag_for_Sri_Lanka:                                            🇱🇰\n",
      ":golf:                                                          ⛳\n",
      ":house_buildings:                                               🏘\n",
      ":large_blue_circle:                                             🔵\n",
      ":office:                                                        🏢\n",
      ":reversed_hand_with_middle_finger_extended:                     🖕\n",
      ":spiral_calendar_pad:                                           🗓\n",
      ":tophat:                                                        🎩\n",
      ":womens:                                                        🚺\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in emoji.EMOJI_ALIAS_UNICODE:\n",
    "    count +=1\n",
    "    if count%50 == 0:\n",
    "        w_s = ' '*(max_len - len(i) + 5)\n",
    "        print(i,w_s , emoji.EMOJI_ALIAS_UNICODE[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorming\n",
    "\n",
    "My first thought: find patterns in the description text of emojis to determine a label.  For instance, if emoji's description includes the word \"face\", it is highly likely it would be categorized as a *mood* more than any other category. Therefore, I would loop over every emoji's description, do a regex search on certain patterns, then go to through if conditions to determine its category.\n",
    "\n",
    "Why I instantly rejected this: Categorizing emojis based on fixed heuristics that I create is wrong because emojis can mean different things depending on the person and circumstance.\n",
    "\n",
    "My second thought: what if I use a pretrained model to work on top of?  [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) comes into mind.  This technique is a popular way to learn word embeddings. Word2Vec converts words into vectors in a space that captures context, syntactic and semantic relations with other words. For instance, vectorizing a set of synonyms with Word2Vec's pretrained model will yield vectors with high cosine similarities (i.e. these vectors roughly point to the same region in this vector space).   \n",
    "\n",
    "\n",
    "How exactly can I leverage this:\n",
    "* Embed the words *place, person, mood, activity, thing, time* to yield six vectors.  These vectors are located somewhere in a 300 dimension space, each with a certain magnitude and direction representing rich information about that word's form, meaning, and context. \n",
    "* Vectorize each emoji with the [Emoji2Vec](https://github.com/uclmr/emoji2vec) pretrained model \n",
    "* Given each *emoji vector*, calculate the cosine similarity between it and each of the six categories. This will output a sequence of six scalars.  Choose the category with the highest similarity score.\n",
    "\n",
    "Why this approach is appealing to me:  emoji categorization is not the result of my own perception of how an emoji ought to be labelled.  Instead, it is the result of capturing its representation in a rich vector space that has been trained on millions of sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['person', 'place', 'thing', 'time', 'activity', 'mood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as gsm\n",
    "e2v = gsm.KeyedVectors.load_word2vec_format('emoji2vec/pre-trained/emoji2vec.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ems = list(emoji.EMOJI_ALIAS_UNICODE.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_vec = []\n",
    "ems_not_working = []\n",
    "for idx, em in enumerate(ems):\n",
    "    try:\n",
    "        v = e2v[em].reshape(1, -1)\n",
    "        ems_vec.append((idx,v))\n",
    "    except:\n",
    "        ems_not_working.append(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ems_not_working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticing a problem with the e2v model, as a third of emojis weren't found in e2v's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2243"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ems_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace\n",
    "\n",
    "def get_max_similarity(v, cats):\n",
    "    \"\"\"\n",
    "    Given emoji vector v and category embeddings cats, function returns the category \n",
    "    with the highest cosine similarity score.\n",
    "    \n",
    "    Input: v -- type: <numpy.ndarray> with shape (300,) with float64 entries; \n",
    "           cats -- type: <dict>. keys are type <str>, values are type <numpy.ndarray> with shape(300,)\n",
    "                                 and float64 entries.\n",
    "    Output: max_cat -- type: <str>\n",
    "    \"\"\"\n",
    "    results = []\n",
    "#     set_trace()\n",
    "    for cat, vec in cats.items():\n",
    "        results.append((cat, cosine_similarity(v, vec)))\n",
    "    max_dis = -1\n",
    "    max_cat = ''\n",
    "    for cat, dis in results:\n",
    "#         set_trace()\n",
    "        if dis > max_dis:\n",
    "            max_dis = dis\n",
    "            max_cat = cat\n",
    "    return max_cat\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "person= model.word_vec('person').reshape(1,-1)\n",
    "place = model.word_vec('place').reshape(1,-1)\n",
    "thing = model.word_vec('thing').reshape(1,-1)\n",
    "activity = model.word_vec('activity').reshape(1,-1)\n",
    "time = model.word_vec('time').reshape(1,-1)\n",
    "mood = model.word_vec('mood').reshape(1,-1)\n",
    "categories = {'person':person, 'place':place, 'thing':thing, 'activity':activity, 'time':time, 'mood':mood}\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'person':person, 'place':place, 'thing':thing, 'activity':activity, 'time':time, 'mood':mood}\n",
    "results = []\n",
    "\n",
    "for idx, vector in ems_vec:\n",
    "    best_cat = get_max_similarity(vector, categories)\n",
    "    results.append((idx, best_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "\n",
    "for idx, cat in results:\n",
    "    new_dict[ems[idx]]=cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🇦🇲 time\n",
      "🇧🇲 place\n",
      "🇰🇭 time\n",
      "🇨🇵 time\n",
      "🇩🇬 person\n",
      "🇫🇴 thing\n",
      "🇬🇱 place\n",
      "🇮🇳 person\n",
      "🎎 thing\n",
      "🇰🇪 time\n",
      "🇱🇺 time\n",
      "🇫🇲 time\n",
      "🇳🇱 thing\n",
      "👌🏾 thing\n",
      "🇵🇳 time\n",
      "🎅🏿 thing\n",
      "🇸🇧 thing\n",
      "🇨🇭 time\n",
      "🇹🇦 time\n",
      "🆚 mood\n",
      "⚗ place\n",
      "🎨 mood\n",
      "👶🏻 person\n",
      "👈🏼 mood\n",
      "🏸 activity\n",
      "🏖 activity\n",
      "⚫ time\n",
      "🔵 time\n",
      "👦🏼 mood\n",
      "🚅 time\n",
      "🍬 mood\n",
      "🧀 person\n",
      "🗜 activity\n",
      "📫 place\n",
      "😖 thing\n",
      "🔄 time\n",
      "🤞 thing\n",
      "🍡 thing\n",
      "😵 mood\n",
      "💧 thing\n",
      "🔌 person\n",
      "😷 person\n",
      "👨‍👨‍👧‍👧 person\n",
      "⛴ mood\n",
      "⛳ time\n",
      "🙏🏻 person\n",
      "🐸 person\n",
      "👧🏾 person\n",
      "💚 thing\n",
      "💂🏼 mood\n",
      "💘 person\n",
      "🍯 thing\n",
      "⌛ thing\n",
      "🔠 thing\n",
      "3️⃣ time\n",
      "😙 mood\n",
      "🗨 time\n",
      "🍭 thing\n",
      "👨🏻 person\n",
      "📣 person\n",
      "📲 person\n",
      "🚠 time\n",
      "💅🏼 thing\n",
      "🚯 thing\n",
      "🐙 person\n",
      "👵🏾 person\n",
      "📖 thing\n",
      "🐂 thing\n",
      "🍑 thing\n",
      "🙇 thing\n",
      "🙅🏿 thing\n",
      "💇🏼 thing\n",
      "🚵🏾 person\n",
      "🙋🏽 person\n",
      "🏄🏿 person\n",
      "🛀🏼 mood\n",
      "👳🏿 person\n",
      "🛐 place\n",
      "🚰 thing\n",
      "🖨 person\n",
      "✊ time\n",
      "🙌🏾 person\n",
      "💞 time\n",
      "🌹 thing\n",
      "🙈 thing\n",
      "🚿 thing\n",
      "🔸 place\n",
      "⚽ activity\n",
      "🗓 activity\n",
      "⏱ time\n",
      "🚟 time\n",
      "🎾 time\n",
      "👍🏾 thing\n",
      "🚊 person\n",
      "👬 person\n",
      "📼 thing\n",
      "🚾 place\n",
      "♿ person\n",
      "😉 thing\n",
      "👢 thing\n",
      "🇦🇽 place\n",
      "🚆 time\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for key in new_dict:\n",
    "    count+=1\n",
    "    if count%15==0:\n",
    "        print(key, new_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach \\#1: Using pre-trained word2vec and emoji2vec models to compare distances between emojis and each categories; the category that renders the highest cosine similarity would be chosen as the category for that emoji.\n",
    "\n",
    "Observable Problems\n",
    "\n",
    "* Mixed labelling with flags\n",
    "* Mixed labelling with people\n",
    "* Mixed labelling with animals\n",
    "* Rejects 1/3 of emojis\n",
    "\n",
    "\n",
    "emoji2vec doesn't seem to provide a solid baseline. Its embedding method for emojies is not very generalizable.\n",
    "\n",
    "Therefore, attempt Approach #2:  Emoji vectors will be created by vectorizing their text descriptions.  Each emoji description has rich data about its meaning: for instance :clapping_hands: contains words that define exactly what that emoji is. I can exploit word2vec here to generate a sequence of word embeddings for each emoji.  Then, I take the average of all word embeddings to get one vector -- this vector will be the emoji vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "smilies = emoji.EMOJI_ALIAS_UNICODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3503"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smilies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(smilies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':mermaid_medium-light_skin_tone:',\n",
       " ':oil_drum:',\n",
       " ':person_bowing_light_skin_tone:',\n",
       " ':person_playing_handball_medium_skin_tone:',\n",
       " ':pool_8_ball:']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1500:2000:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to clean the words in order to feed it into word2vec.  Below represents how I cleaned the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = deepcopy(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3503"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3503"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = [i.strip(':') for i in clean_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = [i.rstrip('0123456789.+\\(\\)') for i in clean_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, word in enumerate(clean_words):\n",
    "    if 'skin' in word:\n",
    "        if 'medium' in word:\n",
    "            crp = word.find('medium')\n",
    "            word= word[:crp]\n",
    "        elif 'dark' in word:\n",
    "            crp = word.find('dark')\n",
    "            word= word[:crp]\n",
    "        elif 'light' in word:\n",
    "            crp = word.find('light')\n",
    "            word= word[:crp]\n",
    "    if '-'in word:\n",
    "        word = word.replace('-', ' ')\n",
    "    if 'facepalming' in word:\n",
    "        word = word.replace('facepalming', 'face palming')\n",
    "    split_words = word.split('_')\n",
    "    words = ' '.join(split_words)\n",
    "    split_words = words.split(' ')\n",
    "    while '' in split_words:\n",
    "        split_words.remove('')\n",
    "    clean_words[idx] = split_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3503"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1st', 'place', 'medal'],\n",
       " ['French', 'Guiana'],\n",
       " ['Mrs.', 'Claus'],\n",
       " ['Taiwan'],\n",
       " ['backhand', 'index', 'pointing', 'right'],\n",
       " ['boxing', 'glove'],\n",
       " ['classical', 'building'],\n",
       " ['detective'],\n",
       " ['family'],\n",
       " ['genie'],\n",
       " ['hourglass', 'done'],\n",
       " ['lobster'],\n",
       " ['man', 'elf'],\n",
       " ['man', 'in', 'suit', 'levitating'],\n",
       " ['man', 'running'],\n",
       " ['mermaid'],\n",
       " ['oil', 'drum'],\n",
       " ['person', 'bowing'],\n",
       " ['person', 'playing', 'handball'],\n",
       " ['pool', '8', 'ball'],\n",
       " ['rice', 'cracker'],\n",
       " ['smirking', 'face'],\n",
       " ['thumbs', 'down'],\n",
       " ['waving', 'hand'],\n",
       " ['woman', 'detective'],\n",
       " ['woman', 'judge'],\n",
       " ['woman', 'shrugging'],\n",
       " ['airplane', 'arriving'],\n",
       " ['clock'],\n",
       " ['flag', 'for', 'Antigua', '&', 'Barbuda'],\n",
       " ['flag', 'for', 'Indonesia'],\n",
       " ['flag', 'for', 'St.', 'Barthélemy'],\n",
       " [],\n",
       " ['ok', 'hand'],\n",
       " ['spiral', 'note', 'pad'],\n",
       " ['worried']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_vec = deepcopy(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED: ['AB', 'button', '(blood', 'type']\n",
      "FAILED: ['A', 'button', '(blood', 'type']\n",
      "FAILED: ['B', 'button', '(blood', 'type']\n",
      "FAILED: ['Cocos', '(Keeling)', 'Islands']\n",
      "FAILED: ['Côte', 'd’Ivoire']\n",
      "FAILED: ['Isle', 'of', 'Man']\n",
      "FAILED: ['Japanese', 'free', 'of', 'charge', 'button']\n",
      "FAILED: ['Japanese', 'not', 'free', 'of', 'charge', 'button']\n",
      "FAILED: ['Myanmar', '(Burma']\n",
      "FAILED: ['ON!', 'arrow']\n",
      "FAILED: ['O', 'button', '(blood', 'type']\n",
      "FAILED: ['Statue', 'of', 'Liberty']\n",
      "FAILED: ['UP!', 'button']\n",
      "FAILED: ['bow', 'and', 'arrow']\n",
      "FAILED: ['cat', 'face', 'with', 'tears', 'of', 'joy']\n",
      "FAILED: ['chequered', 'flag']\n",
      "FAILED: ['cloud', 'with', 'lightning', 'and', 'rain']\n",
      "FAILED: ['couch', 'and', 'lamp']\n",
      "FAILED: ['cut', 'of', 'meat']\n",
      "FAILED: ['diamond', 'with', 'a', 'dot']\n",
      "FAILED: ['doughnut']\n",
      "FAILED: ['ear', 'of', 'corn']\n",
      "FAILED: ['eight', 'o’clock']\n",
      "FAILED: ['eleven', 'o’clock']\n",
      "FAILED: ['face', 'blowing', 'a', 'kiss']\n",
      "FAILED: ['face', 'with', 'tears', 'of', 'joy']\n",
      "FAILED: ['five', 'o’clock']\n",
      "FAILED: ['fork', 'and', 'knife']\n",
      "FAILED: ['fork', 'and', 'knife', 'with', 'plate']\n",
      "FAILED: ['four', 'o’clock']\n",
      "FAILED: ['glass', 'of', 'milk']\n",
      "FAILED: ['hammer', 'and', 'pick']\n",
      "FAILED: ['hammer', 'and', 'wrench']\n",
      "FAILED: ['kaaba']\n",
      "FAILED: ['keycap', '#']\n",
      "FAILED: ['keycap', '*']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['keycap']\n",
      "FAILED: ['man', 'and', 'woman', 'holding', 'hands']\n",
      "FAILED: ['man’s', 'shoe']\n",
      "FAILED: ['map', 'of', 'Japan']\n",
      "FAILED: ['men’s', 'room']\n",
      "FAILED: ['merperson']\n",
      "FAILED: ['merperson']\n",
      "FAILED: ['merperson']\n",
      "FAILED: ['merperson']\n",
      "FAILED: ['merperson']\n",
      "FAILED: ['merperson']\n",
      "FAILED: ['nine', 'o’clock']\n",
      "FAILED: ['nut', 'and', 'bolt']\n",
      "FAILED: ['one', 'o’clock']\n",
      "FAILED: ['pile', 'of', 'poo']\n",
      "FAILED: ['place', 'of', 'worship']\n",
      "FAILED: ['pot', 'of', 'food']\n",
      "FAILED: ['raised', 'back', 'of', 'hand']\n",
      "FAILED: ['raised', 'back', 'of', 'hand']\n",
      "FAILED: ['raised', 'back', 'of', 'hand']\n",
      "FAILED: ['raised', 'back', 'of', 'hand']\n",
      "FAILED: ['raised', 'back', 'of', 'hand']\n",
      "FAILED: ['raised', 'back', 'of', 'hand']\n",
      "FAILED: ['rescue', 'worker’s', 'helmet']\n",
      "FAILED: ['roll', 'of', 'paper']\n",
      "FAILED: ['selfie']\n",
      "FAILED: ['selfie']\n",
      "FAILED: ['selfie']\n",
      "FAILED: ['selfie']\n",
      "FAILED: ['selfie']\n",
      "FAILED: ['selfie']\n",
      "FAILED: ['seven', 'o’clock']\n",
      "FAILED: ['shallow', 'pan', 'of', 'food']\n",
      "FAILED: ['sheaf', 'of', 'rice']\n",
      "FAILED: ['shinto', 'shrine']\n",
      "FAILED: ['sign', 'of', 'the', 'horns']\n",
      "FAILED: ['sign', 'of', 'the', 'horns']\n",
      "FAILED: ['sign', 'of', 'the', 'horns']\n",
      "FAILED: ['sign', 'of', 'the', 'horns']\n",
      "FAILED: ['sign', 'of', 'the', 'horns']\n",
      "FAILED: ['sign', 'of', 'the', 'horns']\n",
      "FAILED: ['six', 'o’clock']\n",
      "FAILED: ['skull', 'and', 'crossbones']\n",
      "FAILED: ['star', 'and', 'crescent']\n",
      "FAILED: ['star', 'of', 'David']\n",
      "FAILED: ['tanabata', 'tree']\n",
      "FAILED: ['ten', 'o’clock']\n",
      "FAILED: ['three', 'o’clock']\n",
      "FAILED: ['twelve', 'o’clock']\n",
      "FAILED: ['two', 'o’clock']\n",
      "FAILED: ['wheel', 'of', 'dharma']\n",
      "FAILED: ['woman’s', 'boot']\n",
      "FAILED: ['woman’s', 'clothes']\n",
      "FAILED: ['woman’s', 'hat']\n",
      "FAILED: ['woman’s', 'sandal']\n",
      "FAILED: ['women’s', 'room']\n",
      "FAILED: ['aquarius']\n",
      "FAILED: ['badminton', 'racquet', 'and', 'shuttlecock']\n",
      "FAILED: ['busstop']\n",
      "FAILED: ['capricorn']\n",
      "FAILED: ['ideograph', 'advantage']\n",
      "FAILED: ['cricket', 'bat', 'and', 'ball']\n",
      "FAILED: ['diamond', 'shape', 'with', 'a', 'dot', 'inside']\n",
      "FAILED: ['bangbang']\n",
      "FAILED: ['dove', 'of', 'peace']\n",
      "FAILED: ['ear', 'of', 'rice']\n",
      "FAILED: ['emoji', 'modifier', 'fitzpatrick', 'type', '1']\n",
      "FAILED: ['emoji', 'modifier', 'fitzpatrick', 'type']\n",
      "FAILED: ['emoji', 'modifier', 'fitzpatrick', 'type']\n",
      "FAILED: ['emoji', 'modifier', 'fitzpatrick', 'type']\n",
      "FAILED: ['emoji', 'modifier', 'fitzpatrick', 'type']\n",
      "FAILED: ['interrobang']\n",
      "FAILED: ['field', 'hockey', 'stick', 'and', 'ball']\n",
      "FAILED: ['fishing', 'pole', 'and', 'fish']\n",
      "FAILED: ['facepunch']\n",
      "FAILED: ['flag', 'for', 'Côte', 'd’Ivoire']\n",
      "FAILED: ['flag', 'for', 'Isle', 'of', 'Man']\n",
      "FAILED: ['fuelpump']\n",
      "FAILED: ['gemini']\n",
      "FAILED: ['heartpulse']\n",
      "FAILED: ['bullettrain', 'side']\n",
      "FAILED: ['bullettrain', 'front']\n",
      "FAILED: ['hocho']\n",
      "FAILED: ['ice', 'hockey', 'stick', 'and', 'puck']\n",
      "FAILED: ['capital', 'abcd']\n",
      "FAILED: ['abcd']\n",
      "FAILED: ['keycap', 'asterisk']\n",
      "FAILED: ['keycap', 'digit', 'eight']\n",
      "FAILED: ['keycap', 'digit', 'five']\n",
      "FAILED: ['keycap', 'digit', 'four']\n",
      "FAILED: ['keycap', 'digit', 'nine']\n",
      "FAILED: ['keycap', 'digit', 'one']\n",
      "FAILED: ['keycap', 'digit', 'seven']\n",
      "FAILED: ['keycap', 'digit', 'six']\n",
      "FAILED: ['keycap', 'digit', 'three']\n",
      "FAILED: ['keycap', 'digit', 'two']\n",
      "FAILED: ['keycap', 'digit', 'zero']\n",
      "FAILED: ['keycap', 'number', 'sign']\n",
      "FAILED: ['keycap', 'ten']\n",
      "FAILED: ['couplekiss']\n",
      "FAILED: ['moyai']\n",
      "FAILED: ['a']\n",
      "FAILED: ['ophiuchus']\n",
      "FAILED: ['hankey']\n",
      "FAILED: ['pisces']\n",
      "FAILED: ['raised', 'hand', 'with', 'part', 'between', 'middle', 'and', 'ring', 'fingers']\n",
      "FAILED: ['sagittarius']\n",
      "FAILED: ['scorpius']\n",
      "FAILED: ['u55b']\n",
      "FAILED: ['u6e']\n",
      "FAILED: ['u7a7a']\n",
      "FAILED: ['star', 'of', 'david']\n",
      "FAILED: ['statue', 'of', 'liberty']\n",
      "FAILED: ['table', 'tennis', 'paddle', 'and', 'ball']\n",
      "FAILED: ['thumbsup']\n",
      "FAILED: ['thumbsdown']\n",
      "FAILED: ['thunder', 'cloud', 'and', 'rain']\n",
      "FAILED: ['grey', 'exclamation']\n",
      "FAILED: ['grey', 'question']\n"
     ]
    }
   ],
   "source": [
    "failed_emojies = []\n",
    "for idx, seq in enumerate(words_to_vec):\n",
    "    vectors = []\n",
    "    for word in seq:\n",
    "        try:\n",
    "            vec = model.word_vec(word)\n",
    "            vectors.append(vec)\n",
    "        except:\n",
    "            failed_emojies.append((idx, seq))\n",
    "#             print(f'FAILED: {seq}')\n",
    "            words_to_vec[idx] = []\n",
    "        words_to_vec[idx]=vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed_emojies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, ['AB', 'button', '(blood', 'type']),\n",
       " (230, ['O', 'button', '(blood', 'type']),\n",
       " (721, ['doughnut']),\n",
       " (909, ['glass', 'of', 'milk']),\n",
       " (1041, ['keycap']),\n",
       " (1495, ['men’s', 'room']),\n",
       " (1879, ['pile', 'of', 'poo']),\n",
       " (2016, ['roll', 'of', 'paper']),\n",
       " (2060, ['shinto', 'shrine']),\n",
       " (2147, ['star', 'of', 'David']),\n",
       " (2675, ['woman’s', 'sandal']),\n",
       " (2839, ['dove', 'of', 'peace']),\n",
       " (2890, ['facepunch']),\n",
       " (3206, ['capital', 'abcd']),\n",
       " (3227, ['keycap', 'digit', 'three']),\n",
       " (3329, ['pisces']),\n",
       " (3442, ['thumbsup'])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_emojies[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since emoji descriptions contain more than one word, each of our emoji samples will contain a collection of word vectors.  Since ultimately I want one vector to compare, I will take their average, coded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathlizard/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for idx, vecs in enumerate(words_to_vec):\n",
    "    total = np.zeros((300))\n",
    "    for v in vecs:\n",
    "        total += v\n",
    "    avg_vec = total/len(vecs)\n",
    "    words_to_vec[idx] = avg_vec\n",
    "print(len(words_to_vec))\n",
    "# print(words_to_vec[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "for i in words_to_vec:\n",
    "    print(len(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3503"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'person':person, 'place':place, 'thing':thing, 'activity':activity, 'time':time, 'mood':mood}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, avg_vec in enumerate(words_to_vec):\n",
    "    avg_vec = avg_vec.reshape(1,-1)\n",
    "#     print(idx, avg_vec[:5])\n",
    "    try:\n",
    "        best_cat= get_max_similarity(avg_vec, categories)\n",
    "        results.append((idx, best_cat))\n",
    "    except:\n",
    "        results.append((idx, 'FAIL'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'place'),\n",
       " (1, 'place'),\n",
       " (2, 'place'),\n",
       " (3, 'thing'),\n",
       " (4, 'place'),\n",
       " (5, 'thing'),\n",
       " (6, 'place'),\n",
       " (7, 'activity'),\n",
       " (8, 'activity'),\n",
       " (9, 'time')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(smilies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':1st_place_medal:',\n",
       " ':2nd_place_medal:',\n",
       " ':3rd_place_medal:',\n",
       " ':AB_button_(blood_type):',\n",
       " ':ATM_sign:',\n",
       " ':A_button_(blood_type):',\n",
       " ':Afghanistan:',\n",
       " ':Albania:',\n",
       " ':Algeria:',\n",
       " ':American_Samoa:']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3503\n",
      "3503\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(clean_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=words)\n",
    "df_2 = pd.DataFrame(data=[clean_words])\n",
    "df_2 = df_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.rename(columns={0:'seq_clean_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={0:'EMOJI_ALIAS_UNICODE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emoji'] = pd.Series(list(smilies.values()), index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VEC_TO_CAT'] = pd.Series([i[1] for i in results], index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SEQ_TO_VEC'] =pd.Series(words_to_vec, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.drop(columns=['SEQ_TO_VEC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOJI_ALIAS_UNICODE</th>\n",
       "      <th>emoji</th>\n",
       "      <th>VEC_TO_CAT</th>\n",
       "      <th>seq_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:1st_place_medal:</td>\n",
       "      <td>🥇</td>\n",
       "      <td>place</td>\n",
       "      <td>[1st, place, medal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:2nd_place_medal:</td>\n",
       "      <td>🥈</td>\n",
       "      <td>place</td>\n",
       "      <td>[2nd, place, medal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:3rd_place_medal:</td>\n",
       "      <td>🥉</td>\n",
       "      <td>place</td>\n",
       "      <td>[3rd, place, medal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:AB_button_(blood_type):</td>\n",
       "      <td>🆎</td>\n",
       "      <td>thing</td>\n",
       "      <td>[AB, button, (blood, type]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:ATM_sign:</td>\n",
       "      <td>🏧</td>\n",
       "      <td>place</td>\n",
       "      <td>[ATM, sign]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EMOJI_ALIAS_UNICODE emoji VEC_TO_CAT              seq_clean_text\n",
       "0         :1st_place_medal:     🥇      place         [1st, place, medal]\n",
       "1         :2nd_place_medal:     🥈      place         [2nd, place, medal]\n",
       "2         :3rd_place_medal:     🥉      place         [3rd, place, medal]\n",
       "3  :AB_button_(blood_type):     🆎      thing  [AB, button, (blood, type]\n",
       "4                :ATM_sign:     🏧      place                 [ATM, sign]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMOJI_ALIAS_UNICODE    54\n",
       "emoji                  54\n",
       "VEC_TO_CAT             54\n",
       "SEQ_TO_VEC             54\n",
       "seq_clean_text         54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.VEC_TO_CAT == 'FAIL'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to repair the \"FAIL\" categories manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMOJI_ALIAS_UNICODE    0\n",
       "emoji                  0\n",
       "VEC_TO_CAT             0\n",
       "SEQ_TO_VEC             0\n",
       "seq_clean_text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.EMOJI_ALIAS_UNICODE.str.contains('merperson'), 'VEC_TO_CAT'] = 'person'\n",
    "\n",
    "df.loc[df.EMOJI_ALIAS_UNICODE.str.contains('selfie'), 'VEC_TO_CAT'] = 'person'\n",
    "\n",
    "df.loc[df.EMOJI_ALIAS_UNICODE.str.contains('keycap'), 'VEC_TO_CAT'] = 'thing'\n",
    "\n",
    "df.loc[df.EMOJI_ALIAS_UNICODE.str.contains('thumbs'), 'VEC_TO_CAT'] = 'person'\n",
    "\n",
    "df.loc[df.EMOJI_ALIAS_UNICODE.str.contains('heart'), 'VEC_TO_CAT'] = 'mood'\n",
    "\n",
    "df.loc[df.EMOJI_ALIAS_UNICODE.str.contains('facepunch'), 'VEC_TO_CAT'] = 'activity'\n",
    "\n",
    "df.loc[df.EMOJI_ALIAS_UNICODE.str.contains('1'), 'VEC_TO_CAT'] = 'activity'\n",
    "\n",
    "df.loc[df.VEC_TO_CAT=='FAIL', 'VEC_TO_CAT']='thing'\n",
    "\n",
    "df.loc[df.VEC_TO_CAT=='FAIL'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for later use\n",
    "# df.to_csv('emoji_dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>EMOJI_ALIAS_UNICODE</th>\n",
       "      <th>emoji</th>\n",
       "      <th>VEC_TO_CAT</th>\n",
       "      <th>seq_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>:1st_place_medal:</td>\n",
       "      <td>🥇</td>\n",
       "      <td>activity</td>\n",
       "      <td>['1st', 'place', 'medal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>:2nd_place_medal:</td>\n",
       "      <td>🥈</td>\n",
       "      <td>place</td>\n",
       "      <td>['2nd', 'place', 'medal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>:3rd_place_medal:</td>\n",
       "      <td>🥉</td>\n",
       "      <td>place</td>\n",
       "      <td>['3rd', 'place', 'medal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>:AB_button_(blood_type):</td>\n",
       "      <td>🆎</td>\n",
       "      <td>thing</td>\n",
       "      <td>['AB', 'button', '(blood', 'type']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>:ATM_sign:</td>\n",
       "      <td>🏧</td>\n",
       "      <td>place</td>\n",
       "      <td>['ATM', 'sign']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       EMOJI_ALIAS_UNICODE emoji VEC_TO_CAT  \\\n",
       "0           0         :1st_place_medal:     🥇   activity   \n",
       "1           1         :2nd_place_medal:     🥈      place   \n",
       "2           2         :3rd_place_medal:     🥉      place   \n",
       "3           3  :AB_button_(blood_type):     🆎      thing   \n",
       "4           4                :ATM_sign:     🏧      place   \n",
       "\n",
       "                       seq_clean_text  \n",
       "0           ['1st', 'place', 'medal']  \n",
       "1           ['2nd', 'place', 'medal']  \n",
       "2           ['3rd', 'place', 'medal']  \n",
       "3  ['AB', 'button', '(blood', 'type']  \n",
       "4                     ['ATM', 'sign']  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Observations:\n",
    "\n",
    "* 54 samples with FAIL labels: part of their description isn't found in word2vec's vocabulary.\n",
    "* mixed labelling with animals\n",
    "* mixed labelling with flags\n",
    "\n",
    "To get a clear sense on the quality of my model, I perform a series of six tests.  \n",
    "\n",
    "I created 6 files, each file named after a category that contains 100 emojies that would likely be labelled as that category.  I define some threshold that represents a baseline.  I choose 60%, meaning 6 of 10 emojies would be labelled correctly.  I choose a low threshold due to the point I mentioned above, that emojies are really subjective.  One may interpret the flexing emoji as a person while another would interpret it as a mood or activity.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person \n",
      " True \n",
      " accuracy:  97.82608695652173\n",
      "thing \n",
      " False \n",
      " accuracy:  28.57142857142857\n",
      "place \n",
      " False \n",
      " accuracy:  19.54022988505747\n",
      "time \n",
      " False \n",
      " accuracy:  47.5\n",
      "activity \n",
      " False \n",
      " accuracy:  7.792207792207792\n",
      "mood \n",
      " False \n",
      " accuracy:  45.348837209302324\n"
     ]
    }
   ],
   "source": [
    "from pdb import set_trace\n",
    "failed_emojis =[]\n",
    "files = ['person.txt', 'thing.txt', 'place.txt', 'time.txt', 'activity.txt', 'mood.txt']\n",
    "df = pd.read_csv('emoji_dict.csv')\n",
    "\n",
    "\n",
    "def extract_emojis(string):\n",
    "    cats = [ file[:-4] for file in files ]\n",
    "    result = dict()\n",
    "    try:\n",
    "        descr = emoji.UNICODE_EMOJI_ALIAS[string]\n",
    "        cat = df[df.EMOJI_ALIAS_UNICODE==descr].VEC_TO_CAT.item()\n",
    "        result[cat] = result.get(cat, []) + [string]\n",
    "    except:\n",
    "        failed_emojis.append((string, idx))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as fp:\n",
    "        distribution= dict()\n",
    "        for line in fp:\n",
    "            res = extract_emojis(line.strip('\\n'))\n",
    "            for key in res:\n",
    "                distribution[key] = distribution.get(key, 0) + 1\n",
    "        test_results.append((file, distribution))\n",
    "\n",
    "\n",
    "NEEDED_THRESHOLD = .6\n",
    "\n",
    "for file, res in test_results:\n",
    "    total = sum(res.values())\n",
    "    cat = file[:-4]\n",
    "    perc = res[cat]/total\n",
    "    print(cat, '\\n', NEEDED_THRESHOLD <= perc, '\\n', 'accuracy: ', perc*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('person.txt', {'person': 45, 'place': 1})\n",
      "('thing.txt',\n",
      " {'activity': 7, 'mood': 10, 'person': 37, 'place': 7, 'thing': 28, 'time': 9})\n",
      "('place.txt',\n",
      " {'activity': 6,\n",
      "  'mood': 18,\n",
      "  'person': 20,\n",
      "  'place': 17,\n",
      "  'thing': 14,\n",
      "  'time': 12})\n",
      "('time.txt', {'activity': 11, 'mood': 4, 'person': 12, 'thing': 15, 'time': 38})\n",
      "('activity.txt',\n",
      " {'activity': 6, 'mood': 12, 'person': 19, 'place': 13, 'thing': 19, 'time': 8})\n",
      "('mood.txt', {'mood': 39, 'person': 9, 'thing': 33, 'time': 5})\n",
      "[None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pp\n",
    "pp([pp((a,b)) for a, b in test_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above statistics show that my model is really good at detecting an emoji representing a person, but is very poor at detecting an emoji that represents an activity or a place. It makes sense that person would perform so highly because a description like \":woman_raising_hand:\" or \":man_teacher:\" would assuredly be close to 'person' in the vector space. On the other hand, the word \"France\" has closest semantic meaning to 'mood'.\n",
    "\n",
    "\n",
    "If I had more time, I would have tuned my model by using EmojiNet's keywords as additional words to apply when embedding the emojies.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
